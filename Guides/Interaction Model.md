# Interaction Model

## Dynamic Decompression via LLM

Imagine an AI-enhanced Obsidian environment:

1. You reference "[[Ulrich's Recursive Insight]]" in another conversation
2. LLM notes ambiguity or curiosity, queries Obsidian
3. Obsidian dynamically sends context nodes ("Musil's Moonlit Walk," "Recursive Embodiment") back to the chat
4. LLM integrates and reflects this deeper context into the ongoing dialogue, expanding layers recursively as needed

## Workflow Guidance

### Iterative Expansion Process

1. **Begin with Central Node**: Start with the minimal "Ulrich Insight" framework
2. **Demand-Driven Creation**: Create secondary nodes only as new ideas emerge or upon explicit context-demand from AI dialogues
3. **Spontaneous Insights**: Add "compressed diamonds" for insights, narrative snippets, or theoretical breakthroughs
4. **Continuous Refinement**: Regularly update with feedback from AI chatsâ€”refining links, decompression depth, and resonance clarity

### Practical Usage Patterns

- **Exploration Mode**: Follow links to discover connections
- **Creation Mode**: Add new nodes when insights emerge
- **Reflection Mode**: Use the graph view to identify patterns and gaps
- **Integration Mode**: Connect new concepts to existing framework

## LLM Integration Techniques

- Use consistent formatting to help LLMs recognize node types
- Include clear connection explanations for context
- Maintain consistent terminology across nodes
- Design for both human and AI readability 